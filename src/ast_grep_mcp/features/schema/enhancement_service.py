"""Enhancement service for Schema.org entity graph analysis.

This module implements the core logic for analyzing existing JSON-LD graphs
and suggesting enhancements based on Schema.org vocabulary, SEO best practices,
and Google Rich Results guidelines.
"""

import copy
import json
import re
import time
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import sentry_sdk

from ast_grep_mcp.core.logging import get_logger
from ast_grep_mcp.features.schema.client import SchemaOrgClient, get_schema_org_client
from ast_grep_mcp.features.schema.enhancement_rules import (
    ENTITY_SUGGESTIONS,
    get_all_properties_for_entity,
    get_property_example,
    get_property_priority,
    get_rich_results_for_property,
)
from ast_grep_mcp.models.schema_enhancement import (
    EnhancementPriority,
    EntityEnhancement,
    GraphEnhancementResult,
    MissingEntitySuggestion,
    PropertyEnhancement,
)

logger = get_logger("schema_enhancement")


# =============================================================================
# Graph Loading Functions
# =============================================================================


def _load_graph_from_source(input_source: str, input_type: str) -> List[Dict[str, Any]]:
    """Load and parse JSON-LD graph from file or directory.

    Args:
        input_source: File path or directory path
        input_type: Either "file" or "directory"

    Returns:
        Flat list of all entities from all graphs

    Raises:
        ValueError: If input_source doesn't exist or is invalid
        json.JSONDecodeError: If JSON parsing fails
    """
    logger.info("load_graph_from_source", source=input_source, type=input_type)

    input_path = Path(input_source)
    if not input_path.exists():
        raise ValueError(f"Input source does not exist: {input_source}")

    entities: List[Dict[str, Any]] = []

    if input_type == "file":
        entities = _load_entities_from_file(input_path)
    elif input_type == "directory":
        entities = _load_entities_from_directory(input_path)
    else:
        raise ValueError(f"Invalid input_type: {input_type}. Must be 'file' or 'directory'")

    logger.info("load_complete", total_entities=len(entities))
    return entities


def _load_entities_from_file(file_path: Path) -> List[Dict[str, Any]]:
    """Load entities from a single JSON file."""
    if not file_path.is_file():
        raise ValueError(f"Expected file but got directory: {file_path}")

    logger.debug("parsing_file", path=str(file_path))
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return _extract_entities_from_data(data)


def _load_entities_from_directory(dir_path: Path) -> List[Dict[str, Any]]:
    """Load entities from all JSON files in a directory."""
    if not dir_path.is_dir():
        raise ValueError(f"Expected directory but got file: {dir_path}")

    logger.debug("scanning_directory", path=str(dir_path))
    json_files = list(dir_path.glob("**/*.json"))

    if not json_files:
        logger.warning("no_json_files_found", path=str(dir_path))
        return []

    logger.info("found_json_files", count=len(json_files))
    entities: List[Dict[str, Any]] = []

    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            entities.extend(_extract_entities_from_data(data))
            logger.debug("parsed_file", file=str(json_file))
        except json.JSONDecodeError as e:
            logger.error("json_parse_error", file=str(json_file), error=str(e))
            continue

    return entities


def _extract_entities_from_data(data: Any) -> List[Dict[str, Any]]:
    """Extract entities from parsed JSON-LD data."""
    entities: List[Dict[str, Any]] = []

    if isinstance(data, dict) and '@graph' in data:
        graph = data['@graph']
        if isinstance(graph, list):
            entities.extend(graph)
        else:
            entities.append(graph)
    elif isinstance(data, dict):
        entities.append(data)
    elif isinstance(data, list):
        entities.extend(data)

    return entities


# =============================================================================
# Entity Analysis Functions
# =============================================================================


async def _analyze_entity(entity: Dict[str, Any], client: SchemaOrgClient) -> EntityEnhancement:
    """Analyze a single entity and suggest property enhancements.

    Args:
        entity: Entity dictionary from JSON-LD graph
        client: Initialized Schema.org client

    Returns:
        EntityEnhancement with suggested properties and scores
    """
    entity_id = entity.get('@id', 'unknown')
    entity_type = _extract_entity_type(entity)

    if not entity_type:
        raise ValueError(f"Entity {entity_id} is missing @type field")

    logger.debug("analyzing_entity", entity_id=entity_id, entity_type=entity_type)

    existing_properties = [key for key in entity.keys() if not key.startswith('@')]
    suggested_properties = await _get_suggested_properties(
        entity_type, existing_properties, client
    )

    return EntityEnhancement(
        entity_id=entity_id,
        entity_type=entity_type,
        existing_properties=existing_properties,
        suggested_properties=suggested_properties,
        validation_issues=[],
        seo_score=0.0
    )


def _extract_entity_type(entity: Dict[str, Any]) -> Optional[str]:
    """Extract the primary entity type from @type field."""
    entity_type_raw = entity.get('@type')
    if not entity_type_raw:
        return None
    if isinstance(entity_type_raw, list):
        first_type = entity_type_raw[0] if entity_type_raw else None
        return str(first_type) if first_type else None
    return str(entity_type_raw)


async def _get_suggested_properties(
    entity_type: str,
    existing_properties: List[str],
    client: SchemaOrgClient
) -> List[PropertyEnhancement]:
    """Get suggested properties for an entity type."""
    try:
        all_properties = await client.get_type_properties(entity_type, include_inherited=True)
    except ValueError:
        logger.warning("type_not_found", entity_type=entity_type)
        all_properties = []

    existing_prop_names = set(existing_properties)
    available_prop_names = {prop['name'] for prop in all_properties}
    missing_prop_names = available_prop_names - existing_prop_names

    suggested: List[PropertyEnhancement] = []
    for prop_name in missing_prop_names:
        prop_details = next((p for p in all_properties if p['name'] == prop_name), None)
        if not prop_details:
            continue

        enhancement = _score_property(prop_name, entity_type, prop_details)
        if enhancement.priority in [
            EnhancementPriority.CRITICAL,
            EnhancementPriority.HIGH,
            EnhancementPriority.MEDIUM
        ]:
            suggested.append(enhancement)

    priority_order = {
        EnhancementPriority.CRITICAL: 0,
        EnhancementPriority.HIGH: 1,
        EnhancementPriority.MEDIUM: 2,
        EnhancementPriority.LOW: 3,
    }
    suggested.sort(key=lambda p: priority_order[p.priority])

    return suggested


def _score_property(
    property_name: str,
    entity_type: str,
    prop_details: Dict[str, Any]
) -> PropertyEnhancement:
    """Score a property's importance and create enhancement suggestion."""
    priority = get_property_priority(entity_type, property_name)
    rich_results = get_rich_results_for_property(property_name)
    google_rich_result = rich_results[0] if rich_results else None

    reason = _build_property_reason(priority, google_rich_result)
    example_value = get_property_example(property_name)
    if example_value is None:
        expected_types = prop_details.get('expectedTypes', [])
        example_value = f"<{expected_types[0]}>" if expected_types else f"<{property_name}>"

    return PropertyEnhancement(
        property_name=property_name,
        expected_types=prop_details.get('expectedTypes', []),
        priority=priority,
        reason=reason,
        example_value=example_value,
        google_rich_result=google_rich_result
    )


def _build_property_reason(priority: EnhancementPriority, google_rich_result: Optional[str]) -> str:
    """Build human-readable reason for property suggestion."""
    reasons = {
        EnhancementPriority.CRITICAL: (
            f"Critical for {google_rich_result}" if google_rich_result
            else "Critical for Schema.org compliance"
        ),
        EnhancementPriority.HIGH: (
            f"Strongly recommended for {google_rich_result}" if google_rich_result
            else "Strongly recommended for SEO"
        ),
        EnhancementPriority.MEDIUM: "Improves discoverability and user experience",
        EnhancementPriority.LOW: "Nice to have for completeness",
    }
    return reasons.get(priority, "")


# =============================================================================
# Reference Validation Functions
# =============================================================================


def _validate_entity_references(entity: Dict[str, Any], all_ids: Set[str]) -> List[str]:
    """Validate that all @id references in an entity exist in the graph."""
    issues: List[str] = []
    entity_id = entity.get('@id', 'unknown')
    references = _find_id_references(entity)

    for ref_id, json_path in references:
        if ref_id not in all_ids:
            issues.append(f"Broken reference at {json_path}: @id '{ref_id}' not found in graph")

    if issues:
        logger.warning("validation_issues_found", entity_id=entity_id, issue_count=len(issues))

    return issues


def _find_id_references(obj: Any, path: str = "") -> List[tuple[str, str]]:
    """Find all @id references in a nested structure."""
    refs: List[tuple[str, str]] = []

    if isinstance(obj, dict):
        if '@id' in obj and len(obj) == 1:
            refs.append((obj['@id'], path))
        else:
            for key, value in obj.items():
                if key == '@id':
                    continue
                new_path = f"{path}.{key}" if path else key
                refs.extend(_find_id_references(value, new_path))
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            new_path = f"{path}[{i}]"
            refs.extend(_find_id_references(item, new_path))

    return refs


# =============================================================================
# Missing Entity Detection Functions
# =============================================================================


def _parse_suggestion_rule(
    rule: str,
    entity_types: Set[str],
    type_counts: Dict[str, int]
) -> bool:
    """Parse and evaluate entity suggestion rules."""
    conditions = [cond.strip() for cond in rule.split(" AND ")]

    for condition in conditions:
        if condition.startswith("NOT has:"):
            entity_type = condition.replace("NOT has:", "").strip()
            if entity_type in entity_types:
                return False
        elif condition.startswith("has:"):
            entity_type = condition.replace("has:", "").strip()
            if entity_type not in entity_types:
                return False
        elif "count:" in condition and ">" in condition:
            match = re.match(r"count:(\w+)>(\d+)", condition)
            if match:
                entity_type = match.group(1)
                threshold = int(match.group(2))
                if type_counts.get(entity_type, 0) <= threshold:
                    return False
            else:
                return False
        else:
            return False

    return True


def _suggest_missing_entities(
    existing_entities: List[Dict[str, Any]],
    _client: SchemaOrgClient
) -> List[MissingEntitySuggestion]:
    """Suggest missing entity types based on graph composition."""
    entity_types: Set[str] = set()
    type_counts: Dict[str, int] = {}

    for entity in existing_entities:
        entity_type_value = entity.get("@type")
        if entity_type_value:
            types = entity_type_value if isinstance(entity_type_value, list) else [entity_type_value]
            for etype in types:
                entity_types.add(etype)
                type_counts[etype] = type_counts.get(etype, 0) + 1

    suggestions: List[MissingEntitySuggestion] = []

    for rule, config in ENTITY_SUGGESTIONS.items():
        if _parse_suggestion_rule(rule, entity_types, type_counts):
            suggestion = MissingEntitySuggestion(
                entity_type=config["suggest"],
                priority=config["priority"],
                reason=config["reason"],
                example=_generate_example_entity(config["suggest"]),
                google_rich_result=config.get("google_rich_result")
            )
            suggestions.append(suggestion)

    priority_order = {
        EnhancementPriority.CRITICAL: 0,
        EnhancementPriority.HIGH: 1,
        EnhancementPriority.MEDIUM: 2,
        EnhancementPriority.LOW: 3,
    }
    suggestions.sort(key=lambda s: priority_order.get(s.priority, 4))

    return suggestions


def _generate_example_entity(
    entity_type: str,
    base_url: str = "https://example.com"
) -> Dict[str, Any]:
    """Generate example JSON-LD structure for an entity type."""
    base_structure: Dict[str, Any] = {
        "@type": entity_type,
        "@id": f"{base_url}#{entity_type.lower()}"
    }

    examples: Dict[str, Dict[str, Any]] = {
        "FAQPage": {
            "mainEntity": [{
                "@type": "Question",
                "name": "What is your most frequently asked question?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "This is the answer to the question."
                }
            }]
        },
        "WebSite": {
            "name": "Example Website",
            "url": base_url,
            "potentialAction": {
                "@type": "SearchAction",
                "target": {
                    "@type": "EntryPoint",
                    "urlTemplate": f"{base_url}/search?q={{search_term_string}}"
                },
                "query-input": "required name=search_term_string"
            }
        },
        "BreadcrumbList": {
            "itemListElement": [
                {"@type": "ListItem", "position": 1, "name": "Home", "item": base_url},
                {"@type": "ListItem", "position": 2, "name": "Category", "item": f"{base_url}/category"}
            ]
        },
        "Review": {
            "reviewRating": {"@type": "Rating", "ratingValue": "5", "bestRating": "5"},
            "author": {"@type": "Person", "name": "Reviewer Name"},
            "reviewBody": "This is an example review.",
            "datePublished": "2025-01-01"
        },
    }

    base_structure.update(examples.get(entity_type, {
        "name": f"Example {entity_type}",
        "description": f"Placeholder for {entity_type} entity"
    }))

    return base_structure


# =============================================================================
# SEO Scoring Functions
# =============================================================================


def _calculate_entity_seo_score(entity_enhancement: EntityEnhancement) -> float:
    """Calculate SEO completeness score for a single entity."""
    PRIORITY_WEIGHTS = {
        EnhancementPriority.CRITICAL: -20,
        EnhancementPriority.HIGH: -10,
        EnhancementPriority.MEDIUM: -5,
        EnhancementPriority.LOW: -2,
    }

    score = 100.0
    for suggestion in entity_enhancement.suggested_properties:
        score += PRIORITY_WEIGHTS.get(suggestion.priority, 0)

    if not entity_enhancement.validation_issues:
        score += 5.0

    return max(0.0, min(100.0, score))


def _calculate_overall_seo_score(
    entity_enhancements: List[EntityEnhancement],
    missing_entities: List[MissingEntitySuggestion],
) -> float:
    """Calculate overall SEO completeness score for the entire graph."""
    MISSING_ENTITY_PENALTIES = {
        EnhancementPriority.CRITICAL: -15,
        EnhancementPriority.HIGH: -10,
        EnhancementPriority.MEDIUM: -5,
        EnhancementPriority.LOW: -2,
    }

    if entity_enhancements:
        avg_entity_score = sum(e.seo_score for e in entity_enhancements) / len(entity_enhancements)
    else:
        avg_entity_score = 50.0

    overall_score = avg_entity_score
    for missing_entity in missing_entities:
        overall_score += MISSING_ENTITY_PENALTIES.get(missing_entity.priority, 0)

    return max(0.0, min(100.0, overall_score))


def _build_priority_summary(
    entity_enhancements: List[EntityEnhancement],
    missing_entities: List[MissingEntitySuggestion],
) -> Dict[str, int]:
    """Build summary of enhancement suggestions by priority level."""
    priority_counts: Dict[str, int] = {"critical": 0, "high": 0, "medium": 0, "low": 0}

    for entity in entity_enhancements:
        for suggestion in entity.suggested_properties:
            priority_counts[suggestion.priority.value] += 1

    for missing_entity in missing_entities:
        priority_counts[missing_entity.priority.value] += 1

    return priority_counts


def _validate_graph_structure(graph_data: Dict[str, Any]) -> List[str]:
    """Validate the structure of a Schema.org JSON-LD graph."""
    issues: List[str] = []

    context = graph_data.get("@context")
    if not context:
        issues.append("Missing @context - should be 'https://schema.org' or similar")
    elif isinstance(context, str):
        valid_contexts = ["https://schema.org", "http://schema.org"]
        if not any(context.startswith(valid) for valid in valid_contexts):
            issues.append(f"Invalid @context '{context}' - should be 'https://schema.org'")

    has_graph = "@graph" in graph_data
    has_type = "@type" in graph_data

    if not has_graph and not has_type:
        issues.append("Missing @graph array or single entity with @type")
    elif has_graph:
        graph = graph_data["@graph"]
        if not isinstance(graph, list):
            issues.append("@graph should be an array of entities")
        elif len(graph) == 0:
            issues.append("Graph contains no entities")

    return issues


# =============================================================================
# Output Generation Functions
# =============================================================================


def _generate_enhanced_graph(
    original_graph: List[Dict[str, Any]],
    entity_enhancements: List[EntityEnhancement],
    missing_entities: List[MissingEntitySuggestion],
    base_url: str = "https://example.com",
) -> Dict[str, Any]:
    """Generate enhanced JSON-LD graph with all suggestions applied."""
    enhanced_graph_array = copy.deepcopy(original_graph)

    for enhancement in entity_enhancements:
        target_entity = next(
            (e for e in enhanced_graph_array if e.get("@id") == enhancement.entity_id),
            None
        )
        if target_entity:
            for prop in enhancement.suggested_properties:
                if prop.property_name not in target_entity:
                    target_entity[prop.property_name] = prop.example_value

    entity_counter = 1
    for missing in missing_entities:
        new_entity = copy.deepcopy(missing.example)
        if "@id" not in new_entity:
            new_entity["@id"] = f"{base_url}#{missing.entity_type.lower()}-{entity_counter}"
            entity_counter += 1
        enhanced_graph_array.append(new_entity)

    return {"@context": "https://schema.org", "@graph": enhanced_graph_array}


def _generate_diff(
    original_graph: List[Dict[str, Any]],
    entity_enhancements: List[EntityEnhancement],
    missing_entities: List[MissingEntitySuggestion],
) -> Dict[str, Any]:
    """Generate diff showing only additions needed to enhance the graph."""
    property_additions: Dict[str, Dict[str, Any]] = {}
    total_properties = 0

    for enhancement in entity_enhancements:
        if enhancement.suggested_properties:
            entity_props = {
                prop.property_name: prop.example_value
                for prop in enhancement.suggested_properties
            }
            property_additions[enhancement.entity_id] = entity_props
            total_properties += len(entity_props)

    new_entities = [copy.deepcopy(m.example) for m in missing_entities]

    return {
        "property_additions": property_additions,
        "new_entities": new_entities,
        "summary": {
            "properties_to_add": total_properties,
            "entities_to_add": len(new_entities),
        }
    }


def _format_result_for_output(
    result: GraphEnhancementResult, output_mode: str
) -> Dict[str, Any]:
    """Format GraphEnhancementResult for JSON serialization."""

    def _serialize_property(prop: PropertyEnhancement) -> Dict[str, Any]:
        prop_dict = asdict(prop)
        prop_dict["priority"] = prop.priority.value
        return prop_dict

    def _serialize_entity(entity: EntityEnhancement) -> Dict[str, Any]:
        return {
            "entity_id": entity.entity_id,
            "entity_type": entity.entity_type,
            "existing_properties": entity.existing_properties,
            "suggested_properties": [_serialize_property(p) for p in entity.suggested_properties],
            "validation_issues": entity.validation_issues,
            "seo_score": entity.seo_score,
        }

    def _serialize_missing(missing: MissingEntitySuggestion) -> Dict[str, Any]:
        missing_dict = asdict(missing)
        missing_dict["priority"] = missing.priority.value
        return missing_dict

    output: Dict[str, Any] = {
        "entity_enhancements": [_serialize_entity(e) for e in result.entity_enhancements],
        "missing_entities": [_serialize_missing(m) for m in result.missing_entities],
        "overall_seo_score": result.overall_seo_score,
        "priority_summary": result.priority_summary,
        "execution_time_ms": result.execution_time_ms,
    }

    if result.global_issues:
        output["global_issues"] = result.global_issues

    if output_mode == "enhanced" and result.enhanced_graph is not None:
        output["enhanced_graph"] = result.enhanced_graph
        output["original_graph_size"] = len(result.original_graph)
    elif output_mode == "diff" and result.diff is not None:
        output["diff"] = result.diff
        output["original_graph_size"] = len(result.original_graph)

    return output


# =============================================================================
# Main Orchestration Function
# =============================================================================


async def analyze_entity_graph(
    input_source: str,
    input_type: str = "file",
    output_mode: str = "analysis"
) -> Dict[str, Any]:
    """Analyze existing JSON-LD graph and suggest enhancements.

    Main entry point for the enhance_entity_graph tool.

    Args:
        input_source: File path or directory path
        input_type: "file" or "directory"
        output_mode: "analysis", "enhanced", or "diff"

    Returns:
        GraphEnhancementResult formatted for the requested output_mode
    """
    start_time = time.time()
    logger.info(
        "analyze_entity_graph_start",
        input_source=input_source,
        input_type=input_type,
        output_mode=output_mode
    )

    try:
        # Load entities from source
        entities = _load_graph_from_source(input_source, input_type)

        if not entities:
            return _format_result_for_output(
                GraphEnhancementResult(
                    original_graph=[],
                    entity_enhancements=[],
                    missing_entities=[],
                    global_issues=["No entities found in input source"],
                    overall_seo_score=0.0,
                    priority_summary={"critical": 0, "high": 0, "medium": 0, "low": 0},
                    execution_time_ms=int((time.time() - start_time) * 1000)
                ),
                output_mode
            )

        # Initialize Schema.org client
        client = get_schema_org_client()

        # Build set of all entity IDs for reference validation
        all_ids: Set[str] = {str(e.get('@id')) for e in entities if e.get('@id')}

        # Analyze each entity
        entity_enhancements: List[EntityEnhancement] = []
        for entity in entities:
            try:
                enhancement = await _analyze_entity(entity, client)
                enhancement.validation_issues = _validate_entity_references(entity, all_ids)
                enhancement.seo_score = _calculate_entity_seo_score(enhancement)
                entity_enhancements.append(enhancement)
            except ValueError as e:
                logger.warning("entity_analysis_failed", error=str(e))
                continue

        # Suggest missing entities
        missing_entities = _suggest_missing_entities(entities, client)

        # Calculate overall scores
        overall_seo_score = _calculate_overall_seo_score(entity_enhancements, missing_entities)
        priority_summary = _build_priority_summary(entity_enhancements, missing_entities)

        # Generate output based on mode
        enhanced_graph = None
        diff = None
        base_url = _extract_base_url(entities)

        if output_mode == "enhanced":
            enhanced_graph = _generate_enhanced_graph(
                entities, entity_enhancements, missing_entities, base_url
            )
        elif output_mode == "diff":
            diff = _generate_diff(entities, entity_enhancements, missing_entities)

        execution_time_ms = int((time.time() - start_time) * 1000)

        result = GraphEnhancementResult(
            original_graph=entities,
            entity_enhancements=entity_enhancements,
            missing_entities=missing_entities,
            global_issues=[],
            overall_seo_score=overall_seo_score,
            priority_summary=priority_summary,
            enhanced_graph=enhanced_graph,
            diff=diff,
            execution_time_ms=execution_time_ms
        )

        logger.info(
            "analyze_entity_graph_complete",
            entity_count=len(entity_enhancements),
            missing_count=len(missing_entities),
            seo_score=overall_seo_score,
            execution_time_ms=execution_time_ms
        )

        return _format_result_for_output(result, output_mode)

    except Exception as e:
        logger.error("analyze_entity_graph_failed", error=str(e))
        sentry_sdk.capture_exception(e, extras={
            "input_source": input_source,
            "input_type": input_type,
            "output_mode": output_mode
        })
        raise


def _extract_base_url(entities: List[Dict[str, Any]]) -> str:
    """Extract base URL from entity @id values."""
    for entity in entities:
        entity_id = entity.get('@id', '')
        if entity_id.startswith('http'):
            # Extract base URL (remove fragment and path after domain)
            parts = entity_id.split('#')[0].split('/')
            if len(parts) >= 3:
                return f"{parts[0]}//{parts[2]}"
    return "https://example.com"
